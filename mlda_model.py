# -*- coding: utf-8 -*-
"""mlda model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1caogYVAC92_qRsCKTSGKyy0o_Ta05N63
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pandas seaborn scikit-learn scipy imbalanced-learn

# Core libraries
from pathlib import Path
import warnings

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

from scipy import stats
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.utils.class_weight import compute_class_weight

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, ConfusionMatrixDisplay, accuracy_score, confusion_matrix

from sklearn.linear_model import LogisticRegression

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import balanced_accuracy_score, f1_score, classification_report

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc

from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight
try:
    from imblearn.over_sampling import SMOTE
except ImportError:
    SMOTE = None
    warnings.warn("imblearn not available. Install with `pip install imbalanced-learn` to enable SMOTE demos.")

sns.set_theme(style="whitegrid", palette="deep")
pd.options.display.float_format = '{:,.2f}'.format

ctg_path = Path("CTG_data_only.xlsx")
assert ctg_path.exists(), f"Expected Excel file at {ctg_path}"


def read_feature_sheet(path: Path, sheet=0):
    """Load the CTG feature sheet, using row 2 as headers and data from row 3 onward."""
    xls = pd.ExcelFile(path)
    if isinstance(sheet, int):
        sheet_name = xls.sheet_names[sheet]
    else:
        sheet_name = sheet
    try:
        df = pd.read_excel(xls, sheet_name=sheet_name, header=1)
    except ValueError:
        df = pd.read_excel(xls, sheet_name=sheet_name, header=0)
    return df



sheet2_raw = read_feature_sheet(ctg_path, sheet=0)


print("Sheets detected:", pd.ExcelFile(ctg_path).sheet_names)
print("Feature sheet shape:", sheet2_raw.shape)

def tidy_sheet(df: pd.DataFrame) -> pd.DataFrame:
    """Drop blank columns/rows, clean names, and deduplicate headers."""
    cleaned = df.copy()
    cleaned.columns = [str(col).strip() for col in cleaned.columns]
    cleaned = cleaned.dropna(axis=0, how='all').dropna(axis=1, how='all')
    cleaned = cleaned.loc[:, ~cleaned.columns.str.contains('^Unnamed', case=False)]
    cleaned = cleaned.loc[:, ~cleaned.columns.duplicated()]
    rename_map = {
        'MSTV': 'mSTV',
        'MLTV': 'mLTV',
        'Variance ': 'Variance',
        'TENDENCY': 'Tendency'
    }
    cleaned = cleaned.rename(columns={k: v for k, v in rename_map.items() if k in cleaned.columns})
    return cleaned


sheet2 = tidy_sheet(sheet2_raw)


target_col = 'NSP'
label_leak_cols = ['CLASS', 'A', 'B', 'C', 'D', 'E', 'AD', 'DE', 'LD', 'FS', 'SUSP']
label_leak_cols = [col for col in label_leak_cols if col in sheet2.columns]


sheet2.head()

"""#Data cleaning

##Missing values
"""

missing_summary = sheet2.isna().sum().sort_values(ascending=False)
missing_summary[missing_summary > 0]

demo_feature = 'ASTV' if 'ASTV' in sheet2.columns else sheet2.select_dtypes(float).columns[0]
demo_df = sheet2[[demo_feature]].copy()
rng = np.random.default_rng(42)
mask_indices = rng.choice(demo_df.index, size=5, replace=False)
demo_df.loc[mask_indices, demo_feature] = np.nan

before_impute = demo_df.loc[mask_indices]
demo_df[demo_feature] = demo_df[demo_feature].fillna(demo_df[demo_feature].median())
after_impute = demo_df.loc[mask_indices]

pd.DataFrame({
    'Index': mask_indices,
    'Before': before_impute.squeeze().values,
    'After': after_impute.squeeze().values
})

"""##Duplicates & Redundant Labels

"""

duplicate_rows = sheet2.duplicated().sum()
duplicate_rows

columns_to_drop = label_leak_cols
print('Dropping label leakage columns:', columns_to_drop)
clean_df = sheet2.drop(columns=columns_to_drop, errors='ignore')
clean_df.head()

"""---
# Feature Engineering

## Encoding & Leakage Removal
"""

#inspect the target column
clean_df[target_col].value_counts(dropna=False)
clean_df[target_col].unique()

#drop or impute missing labels
y = clean_df[target_col]

#drop rows with missing NSP values
mask = y.notna()
y = y.loc[mask]

# 3. Make sure the dtype is consistent (integers)
y = y.astype(int)

#recompute classes and weights
classes = np.unique(y)
weights = compute_class_weight(class_weight='balanced',
                               classes=classes,
                               y=y)

class_weight_dict = dict(zip(classes, weights))
class_weight_dict

"""## Handling Imbalance

"""

feature_cols = [col for col in clean_df.columns if col != target_col]
y = clean_df[target_col].dropna().astype(int)
X = clean_df.loc[y.index, feature_cols]

X_imputed = X.fillna(X.median(numeric_only=True))

if SMOTE is not None:
    smote = SMOTE(random_state=42, sampling_strategy='not majority')
    X_resampled, y_resampled = smote.fit_resample(X_imputed, y)
    print('Original shape:', X.shape, y.value_counts().to_dict())
    print('Resampled shape:', X_resampled.shape, pd.Series(y_resampled).value_counts().to_dict())
else:
    print('SMOTE unavailable. Consider installing imbalanced-learn for oversampling demos.')

"""## Feature Selection

"""

mask = clean_df[target_col].notna()
y = clean_df.loc[mask, target_col].astype(int)
numeric_features = clean_df.select_dtypes(include=[np.number]).columns.drop(target_col)
X_numeric = clean_df.loc[mask, numeric_features].fillna(clean_df[numeric_features].median())
minmax = MinMaxScaler()
X_scaled = minmax.fit_transform(X_numeric)
selector = SelectKBest(score_func=chi2, k=10)
selector.fit(X_scaled, y)
selected_scores = pd.Series(selector.scores_, index=numeric_features).sort_values(ascending=False)
selected_scores.head(15)

"""---
# Advanced EDA / ML Preparation

## Train/Test Split
"""

X_train, X_test, y_train, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled
)

print('Train shape:', X_train.shape)
print('Test shape:', X_test.shape)

pd.DataFrame({
    'Split': ['Train', 'Test'],
    'Normal (1)': [np.mean(y_train == 1), np.mean(y_test == 1)],
    'Suspect (2)': [np.mean(y_train == 2), np.mean(y_test == 2)],
    'Pathologic (3)': [np.mean(y_train == 3), np.mean(y_test == 3)]
}).rename(columns=lambda c: c if c == 'Split' else f'{c} proportion')

# Stratified train/test split and class weights
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)
classes = np.unique(y_train)
class_weight_values = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
class_weight_dict = dict(zip(classes, class_weight_values))
sample_weight_train = compute_sample_weight(class_weight='balanced', y=y_train)

distribution = (
    pd.DataFrame({
        'Split': ['Train', 'Test'],
        'Size': [len(y_train), len(y_test)],
        'Normal (1)': [np.mean(y_train == 1), np.mean(y_test == 1)],
        'Suspect (2)': [np.mean(y_train == 2), np.mean(y_test == 2)],
        'Pathologic (3)': [np.mean(y_train == 3), np.mean(y_test == 3)]
    })
    .rename(columns=lambda c: c if c in {'Split', 'Size'} else f'{c} proportion')
)
distribution, class_weight_dict

"""#Model"""

results = []

def evaluate_model(name, estimator, X_train, y_train, X_test, y_test, fit_kwargs=None, display_report=False):
    fit_kwargs = fit_kwargs or {}
    estimator.fit(X_train, y_train, **fit_kwargs)
    y_pred = estimator.predict(X_test)
    bal_acc = balanced_accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='macro')
    results.append({'Model': name, 'Balanced Accuracy': bal_acc, 'F1 Macro': f1})
    print(f"{name} — Balanced Accuracy: {bal_acc:.3f}, Macro F1: {f1:.3f}")
    if display_report:
        print(classification_report(y_test, y_pred, digits=3))
    disp = ConfusionMatrixDisplay.from_predictions(
        y_test, y_pred, display_labels=sorted(classes), normalize='true', cmap='Blues'
    )
    disp.ax_.set_title(f"{name} — Normalized Confusion Matrix")
    plt.show()
    return estimator, y_pred

classes

"""##Gradient Boosting"""

gb_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('gb', GradientBoostingClassifier(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=3,
        random_state=42
    ))
])
_ = evaluate_model('Gradient Boosting (Weighted Features + SMOTE)', gb_pipeline, X_resampled, y_resampled, X_test, y_test, display_report=True)

top_features = ['AC', 'ASTV', 'ALTV', 'DP']
weight_factor = 2

X_resampled_weighted = X_resampled.copy()
for feat in top_features:
    if feat in X_resampled_weighted.columns:
        X_resampled_weighted[feat] *= weight_factor
    else:
        print(f"Warning: Feature '{feat}' not found in resampled training data.")

#sample weights
sample_weight_resampled = compute_sample_weight(class_weight='balanced', y=y_resampled)
gb_fit_kwargs_resampled = {'gb__sample_weight': sample_weight_resampled}

#model pipeline
gb_pipeline.fit(X_resampled_weighted, y_resampled, **gb_fit_kwargs_resampled)

#input data
excel_file_path = 'input.xlsx'
input_df = pd.read_excel(excel_file_path)

trained_features = X_resampled.columns
input_df = input_df.reindex(columns=trained_features)

#predict with model
predictions = gb_pipeline.predict(input_df)
probabilities = gb_pipeline.predict_proba(input_df)

#map classes to their names
class_label_map = {
    1: 'Normal',
    2: 'Suspect',
    3: 'Pathologic'
}

#output predictions
for i, pred in enumerate(predictions):
    class_name = class_label_map.get(pred, f"Unknown ({pred})")
    print(f"Input {i+1} predicted class: {class_name}")

model_classes = gb_pipeline.named_steps['gb'].classes_

for i, prob in enumerate(probabilities):
    print(f"Input {i+1} class probabilities:")
    for cls, p in zip(model_classes, prob):
        class_name = class_label_map.get(cls, f"Unknown ({cls})")
        print(f"  {class_name}: {p * 100:.2f}%")

