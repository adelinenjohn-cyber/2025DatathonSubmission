# -*- coding: utf-8 -*-
"""mlda eda

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gOBHnYV_wIQ1Abmj6GQVrdnFR609ON4R
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pandas seaborn scikit-learn scipy imbalanced-learn

# Core libraries
from pathlib import Path
import warnings

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

from scipy import stats
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.utils.class_weight import compute_class_weight

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, ConfusionMatrixDisplay, accuracy_score, confusion_matrix

from sklearn.linear_model import LogisticRegression

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import balanced_accuracy_score, f1_score, classification_report

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc

from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight
try:
    from imblearn.over_sampling import SMOTE
except ImportError:
    SMOTE = None
    warnings.warn("imblearn not available. Install with `pip install imbalanced-learn` to enable SMOTE demos.")

sns.set_theme(style="whitegrid", palette="deep")
pd.options.display.float_format = '{:,.2f}'.format

ctg_path = Path("CTG_data_only.xlsx")
assert ctg_path.exists(), f"Expected Excel file at {ctg_path}"


def read_feature_sheet(path: Path, sheet=0):
    """Load the CTG feature sheet, using row 2 as headers and data from row 3 onward."""
    xls = pd.ExcelFile(path)
    if isinstance(sheet, int):
        sheet_name = xls.sheet_names[sheet]
    else:
        sheet_name = sheet
    try:
        df = pd.read_excel(xls, sheet_name=sheet_name, header=1)
    except ValueError:
        df = pd.read_excel(xls, sheet_name=sheet_name, header=0)
    return df



sheet2_raw = read_feature_sheet(ctg_path, sheet=0)


print("Sheets detected:", pd.ExcelFile(ctg_path).sheet_names)
print("Feature sheet shape:", sheet2_raw.shape)

def tidy_sheet(df: pd.DataFrame) -> pd.DataFrame:
    """Drop blank columns/rows, clean names, and deduplicate headers."""
    cleaned = df.copy()
    cleaned.columns = [str(col).strip() for col in cleaned.columns]
    cleaned = cleaned.dropna(axis=0, how='all').dropna(axis=1, how='all')
    cleaned = cleaned.loc[:, ~cleaned.columns.str.contains('^Unnamed', case=False)]
    cleaned = cleaned.loc[:, ~cleaned.columns.duplicated()]
    rename_map = {
        'MSTV': 'mSTV',
        'MLTV': 'mLTV',
        'Variance ': 'Variance',
        'TENDENCY': 'Tendency'
    }
    cleaned = cleaned.rename(columns={k: v for k, v in rename_map.items() if k in cleaned.columns})
    return cleaned


sheet2 = tidy_sheet(sheet2_raw)


target_col = 'NSP'
label_leak_cols = ['CLASS', 'A', 'B', 'C', 'D', 'E', 'AD', 'DE', 'LD', 'FS', 'SUSP']
label_leak_cols = [col for col in label_leak_cols if col in sheet2.columns]


sheet2.head()

"""#EDA"""

sheet2_shape = sheet2.shape
sheet2_info = sheet2.info()
sheet2.describe(include='all').T.head(30)

"""##Target Variable NSP
- **1** = Normal
- **2** = Suspect
- **3** = Pathologic

"""

class_counts = sheet2[target_col].value_counts().sort_index()
class_props = class_counts / class_counts.sum()


fig, ax = plt.subplots(figsize=(6, 4))
sns.barplot(x=class_counts.index.astype(int), y=class_counts.values, ax=ax)
ax.set_xlabel('NSP Class')
ax.set_ylabel('Count')
ax.set_title('NSP Class Distribution')
for index, value in enumerate(class_counts.values):
    label = f"{value}\n{class_props.iloc[index]:.1%}"
    ax.text(index, value + 5, label, ha='center')
plt.tight_layout()
plt.show()


pd.DataFrame({
    'Class': class_counts.index.astype(int),
    'Count': class_counts.values,
    'Proportion': class_props.values
})

"""##Features

- **Physiological signals:** AC, FM, UC, DL, DS, DP.
- **Variability metrics:** ASTV, mSTV, ALTV, mLTV.
- **Histogram descriptors:** Width, Min, Max, Nmax, Nzeros, Mode, Mean, Median, Variance, Tendency.
- **Timing:** b (start), e (end).
- **Baseline references:** LBE (expert), LB (SisPorto).
"""

feature_groups = {
    'physiological': ['AC', 'FM', 'UC', 'DL', 'DS', 'DP'],
    'variability': ['ASTV', 'mSTV', 'ALTV', 'mLTV'],
    'histogram': ['Width', 'Min', 'Max', 'Nmax', 'Nzeros', 'Mode', 'Mean', 'Median', 'Variance', 'Tendency'],
    'timing': ['b', 'e'],
    'baseline': ['LBE', 'LB']
}

available_features = {group: [col for col in cols if col in sheet2.columns]
                      for group, cols in feature_groups.items()}
available_features



"""#Data cleaning

##Missing values
"""

missing_summary = sheet2.isna().sum().sort_values(ascending=False)
missing_summary[missing_summary > 0]

demo_feature = 'ASTV' if 'ASTV' in sheet2.columns else sheet2.select_dtypes(float).columns[0]
demo_df = sheet2[[demo_feature]].copy()
rng = np.random.default_rng(42)
mask_indices = rng.choice(demo_df.index, size=5, replace=False)
demo_df.loc[mask_indices, demo_feature] = np.nan

before_impute = demo_df.loc[mask_indices]
demo_df[demo_feature] = demo_df[demo_feature].fillna(demo_df[demo_feature].median())
after_impute = demo_df.loc[mask_indices]

pd.DataFrame({
    'Index': mask_indices,
    'Before': before_impute.squeeze().values,
    'After': after_impute.squeeze().values
})

"""##Outliers

"""

outlier_feature = 'DP' if 'DP' in sheet2.columns else available_features['physiological'][0]
fig, ax = plt.subplots(figsize=(6, 4))
sns.boxplot(x=sheet2[outlier_feature], ax=ax)
ax.set_title(f'Outlier Check: {outlier_feature}')
plt.tight_layout()
plt.show()

dp_scores = stats.zscore(sheet2[outlier_feature].dropna())
outlier_mask = np.abs(dp_scores) > 3
outlier_indices = sheet2[outlier_feature].dropna().index[outlier_mask]
sheet2.loc[outlier_indices, [outlier_feature, target_col]].head()

sheet2[outlier_feature].value_counts().head()

"""##Duplicates & Redundant Labels

"""

duplicate_rows = sheet2.duplicated().sum()
duplicate_rows

columns_to_drop = label_leak_cols
print('Dropping label leakage columns:', columns_to_drop)
clean_df = sheet2.drop(columns=columns_to_drop, errors='ignore')
clean_df.head()

"""#EDA"""

stats_cols = [col for col in ['AC', 'DL'] if col in clean_df.columns]
group_summary = clean_df.groupby(target_col)[stats_cols].agg(['median', 'mean']).round(2)
group_summary

clean_df.describe().T

if {'LB', 'ASTV'}.issubset(clean_df.columns):
    baseline_bins = pd.cut(clean_df['LB'], bins=[0, 110, 160, np.inf],
                          labels=['Bradycardic', 'Normal baseline', 'Tachycardic'])
    variability_alert = pd.cut(clean_df['ASTV'], bins=[0, 10, 25, np.inf],
                               labels=['<10% (worrisome)', '10-25% (borderline)', '>25% (reassuring)'])
    baseline_ct = pd.crosstab(baseline_bins, clean_df[target_col])
    variability_ct = pd.crosstab(variability_alert, clean_df[target_col])
    display(baseline_ct)
    display(variability_ct)
else:
    print('Baseline or ASTV columns missing from clean_df.')

"""## Distribution plots"""

fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)
for ax, feature in zip(axes, ['mSTV', 'mLTV']):
    if feature not in clean_df.columns:
        ax.set_visible(False)
        continue
    sns.histplot(data=clean_df, x=feature, hue=target_col, kde=True,
                 common_norm=False, ax=ax, palette='deep')
    ax.set_title(f'Distribution of {feature}')
plt.tight_layout()
plt.show()

"""##Boxplots by class"""

box_features = [col for col in ['DL', 'DS', 'DP'] if col in clean_df.columns]
plt.figure(figsize=(6, 4 * len(box_features)))
for idx, feature in enumerate(box_features, start=1):
    ax = plt.subplot(len(box_features), 1, idx)
    sns.boxplot(data=clean_df, x=target_col, y=feature, ax=ax)
    ax.set_title(f'{feature} vs NSP')
plt.tight_layout()
plt.show()

"""##Correlation Heatmap"""

corr_features = clean_df.select_dtypes(include=[np.number]).drop(columns=[target_col])
corr_matrix = corr_features.corr(method='pearson')
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, cmap='coolwarm', center=0, square=True)
plt.title('Feature Correlation Heatmap')
plt.show()

corr_pairs = corr_matrix.abs().where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
corr_pairs.stack().sort_values(ascending=False).head(10)

"""## Featureâ€“Target Relationship"""

corr_features = clean_df.select_dtypes(include=[np.number]).drop(columns=[target_col])

# filter out columns with zero or near-zero variance
non_constant_cols = corr_features.columns[corr_features.nunique() > 1]
corr_features = corr_features[non_constant_cols]

anova_results = []
for feature in corr_features.columns:
    groups = [vals.dropna().values for _, vals in clean_df.groupby(target_col)[feature]]
    if len([g for g in groups if len(g) > 0]) < 2:
        continue
    try:
        h_stat, p_value = stats.kruskal(*groups)
        anova_results.append((feature, h_stat, p_value))
    except ValueError:
        # handles any remaining pathological cases
        continue

anova_df = (pd.DataFrame(anova_results, columns=['Feature', 'H-statistic', 'p-value'])
              .sort_values('H-statistic', ascending=False))
anova_df.head(10)

"""AC, AS/ALTV, and DP are top discriminators.

---
# Feature Engineering

## Encoding & Leakage Removal
"""

#inspect the target column
clean_df[target_col].value_counts(dropna=False)
clean_df[target_col].unique()

#drop or impute missing labels
y = clean_df[target_col]

#drop rows with missing NSP values
mask = y.notna()
y = y.loc[mask]

# 3. Make sure the dtype is consistent (integers)
y = y.astype(int)

#recompute classes and weights
classes = np.unique(y)
weights = compute_class_weight(class_weight='balanced',
                               classes=classes,
                               y=y)

class_weight_dict = dict(zip(classes, weights))
class_weight_dict

"""## Handling Imbalance

"""

feature_cols = [col for col in clean_df.columns if col != target_col]
y = clean_df[target_col].dropna().astype(int)
X = clean_df.loc[y.index, feature_cols]

# Median-impute any residual NaNs so SMOTE has a dense matrix to work with
X_imputed = X.fillna(X.median(numeric_only=True))

if SMOTE is not None:
    smote = SMOTE(random_state=42, sampling_strategy='not majority')
    X_resampled, y_resampled = smote.fit_resample(X_imputed, y)
    print('Original shape:', X.shape, y.value_counts().to_dict())
    print('Resampled shape:', X_resampled.shape, pd.Series(y_resampled).value_counts().to_dict())
else:
    print('SMOTE unavailable. Consider installing imbalanced-learn for oversampling demos.')

"""## Feature Selection

"""

mask = clean_df[target_col].notna()
y = clean_df.loc[mask, target_col].astype(int)
numeric_features = clean_df.select_dtypes(include=[np.number]).columns.drop(target_col)
X_numeric = clean_df.loc[mask, numeric_features].fillna(clean_df[numeric_features].median())
minmax = MinMaxScaler()
X_scaled = minmax.fit_transform(X_numeric)
selector = SelectKBest(score_func=chi2, k=10)
selector.fit(X_scaled, y)
selected_scores = pd.Series(selector.scores_, index=numeric_features).sort_values(ascending=False)
selected_scores.head(15)

